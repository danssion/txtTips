[[[[[ how to learn ]]]]]
https://www.zhihu.com/question/26006703

[[[[[ 定义 ]]]]]
导数是什么，无非就是变化率呗
导数其实就是变化率，那么偏导数是什么？偏导数无非就是多个变量的时候，针对某个变量的变化率呗。计算偏导数的时候，其他变量都可以看成常量.

深度学习的概念由Hinton等人于2006年提出，因其通过多层架构实现了抽象认知的学习能力，Hinton命名为“深度学习”
深度学习就是一种特征学习方法，把原始数据通过一些简单的非线性的模型转成为更高层次的更加抽象的表达。通过足够多的转换组合，非常复杂的函数也能被学习。

神经网络算法的核心就是：计算、连接、评估、纠错、疯狂培训。
#http://ufldl.stanford.edu/wiki/index.php/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C
神经网络最左边的一层叫做输入层，最右的一层叫做输出层（本例中，输出层只有一个节点）。中间所有节点组成的一层叫做隐藏层

#http://www.cnblogs.com/rgvb178/p/6055213.html
激活函数:激活函数不是真的要去激活什么。在神经网络中，激活函数的作用是能够给神经网络加入一些非线性因素，使得神经网络可以更好地解决较为复杂的问题。

饱和:
当一个激活函数h(x)满足 lim (n→+∞) h′(x)=0 称之为右饱和。
激活函数h(x)满足 lim (n→−∞)h′(x)=0 之为左饱和
当一个激活函数，既满足左饱和又满足又饱和时，我们称之为饱和。

硬饱和与软饱和:
对任意的xx，如果存在常数cc，当x>cx>c时恒有 h′(x)=0h′(x)=0则称其为右硬饱和，当x<cx<c时恒 有h′(x)=0h′(x)=0则称其为左硬饱和。若既满足左硬饱和，又满足右硬饱和，则称这种激活函数为硬饱和。但如果只有在极限状态下偏导数等于0的函数，称之为软饱和。

tanh函数:
tanh(x)= (1−e−2x) / 1+e−2x

ReLU:
ReLU是最近几年非常受欢迎的激活函数。被定义为
y= {0   (x≤0)
   {x   (x>0)

#https://baike.baidu.com/item/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0/6011241?fr=aladdin 
似然函数:
  似然函数是一种关于统计模型参数的函数。
  概率描述了已知参数时的随机变量的输出结果；似然则用来描述已知随机变量输出结果时，未知参数的可能取值。
“似然性”与“或然性”或“概率”意思相近，都是指某种事件发生的可能性，但是在统计学中，“似然性”和“或然性”或“概率”又有明确的区分。概率用于在已知一些参数的情况下，预测接下来的观测所得到的结果，而似然性则是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。

支持向量机/support vector machine (SVM)
#https://www.zhihu.com/question/21094489
假设我们要通过三八线把实心圈和空心圈分成两类。
那么有无数多条线可以完成这个任务。
在SVM中，我们寻找一条最优的分界线使得它到两边的margin都最大。
在这种情况下边缘加粗的几个数据点就叫做support vector，这也是这个分类算法名字的来源。


卷积网络(convolutional network)(LeCun, 1989)，也叫做 卷积神经网络(con- volutional neural network, CNN)，是一种专门用来处理具有类似网格结构的数据的 神经网络。

BP神经网络
误差反向传播算法(Error Back Propagation Training)，简称BP,人们把采用这种算法进行误差校正的多层前馈网络称为BP网。
BP神经网络的计算过程由正向计算过程和反向计算过程组成。正向传播过程，输入模式从输入层经隐单元层逐层处理，并转向输出层，每～层神经元的状态只影响下一层神经元的状态。如果在输出层不能得到期望的输出，则转入反向传播，将误差信号沿原来的连接通路返回，通过修改各神经元的权值，使得误差信号最小。


Word2Vec
#http://www.sohu.com/a/128794834_211120
基本思想是把自然语言中的每一个词，表示成一个统一意义统一维度的短向量。


循环神经网络(RNN, Recurrent Neural Networks)
#http://blog.csdn.net/heyongluoyao8/article/details/48636251
即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。

LSTM(Long Short-Term Memory Network，长短时记忆神经网络)
#http://blog.csdn.net/Dark_Scope/article/details/47056361
原生的RNN会遇到一个很大的问题，叫做 The vanishing gradient problem for RNNs，也就是后面时间的节点对于前面时间的节点感知力下降，也就是忘事儿,RNN解决这个问题用到的就叫LSTM，简单来说就是你不是忘事儿吗？我给你拿个小本子把事记上，好记性不如烂笔头嘛，所以LSTM引入一个核心元素就是Cell。

Clockwork RNNs(CW-RNNs)
CW-RNNs也是一个RNNs的改良版本，是一种使用时钟频率来驱动的RNNs。它将隐藏层分为几个块(组，Group/Module)，每一组按照自己规定的时钟频率对输入进行处理。

[[[[[ 优化算法 ]]]]]
http://blog.csdn.net/acdreamers/article/details/41413787
Hessian矩阵：在多元函数中求极值的方法类似，只是在判断凹凸性这里引入了一个矩阵，是一个多元函数的二阶偏导数构成的方阵，描述了函数的局部曲率，常用于牛顿迭代法解决优化问题。
拟牛顿法: 基本思想是在牛顿法中用Hessian矩阵的某个近似矩阵来代替它。
拟牛顿算法: BFGS算法。它是Broyden，Fletcher, Goldfarb，Shanno四位牛人发明出来到现在的40多年时间里，它仍然被认为是最好的拟牛顿算法。

http://blog.csdn.net/ACdreamers/article/details/44728041
L-BFGS: 有限内存BFGS。


[[[[[ 深度学习框架与应用 ]]]]]
Caffe 伯克利分校C++编写,主要用户卷积神经网络的学习，在视觉领域应用广泛

Torch，facebook开源。

Tensorflow Google C++ 

Keras 基于Theano 或 TensorFLow作为后端的深度学习框架。
























